from openai import OpenAI


def model_api_client() -> object:
    """Returns an instance of the OpenAI API client."""
    return OpenAI(
        api_key="KG6Dqaqx2APgaNYbqOCrRI5aBXitjbvO",
        base_url="https://api.lemonfox.ai/v1",
    )


def prompt_llm(client: object, prompt: str = "", role: str = "", temperature: int = 0.3) -> str:
    """Interacts with the LLM model to generate a response given the
    prompt and fillers.

    This function uses the OpenAI API client to interact with the LLM model.
    It sends a chat completion request to the model with a system message and a user message.
    The system message contains the role of the user and the user message is generated by filling the prompt with the provided fillers.
    The model then generates a response based on these messages.

    Args:
        client (object): An instance of the OpenAI API client.
        prompt (str): The prompt to be used for generating the response. This is a string that contains placeholders for the fillers.
        role (str): The role of the user. This is used in the system message sent to the model.
        temperature (int): The temperature parameter for the model. This controls the randomness of the generated response.

    Returns:
        str: The generated response from the LLM model.
    """
    return client.chat.completions.create(
        messages=[
            {"role": "system", "content": f"{role}"},
            {"role": "user", "content": f"{prompt}"},
        ],
        model="mixtral-chat",
        temperature=temperature,
    )