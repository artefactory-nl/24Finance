{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sh ../bin/install_requirements_databricks.sh\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "repo_path = os.path.abspath(os.path.join(cwd, '..'))\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "\n",
    "import chromadb as db \n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    "    HuggingFaceEmbeddings,\n",
    ")\n",
    "import pandas as pd\n",
    "from lib.utils import clean_scraped_text\n",
    "from lib.vector.structure import build_vector_db_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from lib.scraping.scrap import collect_rss_feed, extract_news_content_from_url_to_dataframe, load_rss_urls_from_config\n",
    "\n",
    "rss_urls = load_rss_urls_from_config(os.path.join(repo_path, 'config', 'rss_urls.yaml'))\n",
    "\n",
    "rss_feed_df = collect_rss_feed(rss_urls)\n",
    "rss_feed_df.to_csv(os.path.join(repo_path, 'data', 'rss_feed_df.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss_feed_df = extract_news_content_from_url_to_dataframe(rss_feed_df, url_column = 'Link', output_column = 'Content')\n",
    "rss_feed_df.to_csv(os.path.join(repo_path, 'data', 'rss_feed_with_content_df.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_data = pd.read_csv(os.path.join(repo_path, 'data', 'rss_feed_with_content_df.csv')).dropna().reset_index()\n",
    "metadatas_cols = ['Published','Link','Title','Source','Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-l6-v2\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = os.path.join(os.path.dirname(os.path.abspath(\"\")), \"data\", \"vector_db\")\n",
    "chroma_client = db.PersistentClient(path=db_path)\n",
    "collection_name = \"news\"\n",
    "if collection_name not in [c.name for c in chroma_client.list_collections()]:\n",
    "    chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"hnsw:space\": \"cosine\"},\n",
    "        embedding_function=embedding_model,\n",
    "    )\n",
    "    collection_one = chroma_client.get_collection(name=collection_name)\n",
    "    vect_db_structure = build_vector_db_structure(news_data, metadatas_cols, 'index', 'Content')\n",
    "    collection_one.add(\n",
    "        documents=vect_db_structure['datas'],\n",
    "        metadatas=vect_db_structure['metadatas'],\n",
    "        ids=vect_db_structure['ids']\n",
    "    )\n",
    "elif UPDATE==True:\n",
    "    collection_one = chroma_client.get_collection(name=collection_name)\n",
    "    news_data['index'] = news_data['index'] + max([int(id) for id in collection_one.get()['ids']])\n",
    "    vect_db_structure = build_vector_db_structure(news_data, metadatas_cols, 'index', 'Content')\n",
    "\n",
    "\n",
    "else:\n",
    "    collection_one = chroma_client.get_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "langchain_chroma = Chroma(\n",
    "    client=chroma_client,\n",
    "    collection_name=\"news\",\n",
    "    embedding_function=embedding_model,\n",
    ")\n",
    "\n",
    "print(\"There are\", langchain_chroma._collection.count(), \"in the collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"Tesla stocks\"\n",
    "docs_chroma = langchain_chroma.similarity_search_with_score(query, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_chroma[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artefact-hackathon-team-04",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
